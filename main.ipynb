{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Until we get real data, make synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporary: Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=200, n_features=200, random_state=42)\n",
    "data = np.concatenate((X, y.reshape(-1, 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "np.random.seed(42)\n",
    "states = np.random.randint(low = 0, high = 1000000, size=(100,)) # numpy array with our 100 random states\n",
    "data_splits = {}\n",
    "for rst in states:\n",
    "    data_splits[rst] = preprocess.preprocess(data, rst, selection=None, extraction='umap', k=5)\n",
    "    # k: [1, 10, 20, ..., 120, 130, 140, 150, 160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling as models\n",
    "model_names = [\"dt\", \"rf\", \"gb\", \"xgb\", \"lgb\", \"et\", \"ab\", \"lr\", \"lr1\", \"lr2\", \"lre\", \"lsv\", \"nlsv\", \"knn\", \"lda\", \"gnb\", \"mlp\"]\n",
    "\n",
    "\n",
    "model_res = {} # key as model type, then sub dictionary with rst as key and predicted classes & model itself ex: model_res[\"dt\"][rst] --> (y_pred_dt, dt) \n",
    "for model in model_names:\n",
    "    model_res[model] = {}\n",
    "\n",
    "for state in data_splits.keys():    \n",
    "    model_res[\"dt\"][state] = models.model_decisiontree(*data_splits[state]) # decision tree\n",
    "    model_res[\"rf\"][state] = models.model_randomforest(*data_splits[state]) # random forest\n",
    "    model_res[\"gb\"][state] = models.model_gradientboosting(*data_splits[state]) # grad boosting\n",
    "    model_res['xgb'][state] = models.model_extremegb(*data_splits[state])\n",
    "    model_res[\"lgb\"][state] = models.model_lightgb(*data_splits[state])\n",
    "    model_res[\"et\"][state] = models.model_extratrees(*data_splits[state])\n",
    "    model_res[\"ab\"][state] = models.model_adaboost(*data_splits[state])\n",
    "    model_res[\"lr\"][state] = models.model_logisticregression(*data_splits[state])\n",
    "    model_res[\"lr1\"][state] = models.model_lassoregularization(*data_splits[state])\n",
    "    model_res[\"lr2\"][state] = models.model_ridgeRegularization(*data_splits[state])\n",
    "    model_res[\"lre\"][state] = models.model_elasticNetRegularization(*data_splits[state])\n",
    "    model_res[\"lsv\"][state] = models.model_linearSupportVector(*data_splits[state])\n",
    "    model_res[\"nlsv\"][state] = models.model_nonLinearSupportVector(*data_splits[state])\n",
    "    model_res[\"knn\"][state] = models.model_kNearestNeighbor(*data_splits[state])\n",
    "    model_res[\"lda\"][state] = models.model_linearDiscriminantAnalysis(*data_splits[state])\n",
    "    model_res[\"gnb\"][state] = models.model_gaussianNaiveBayes(*data_splits[state])\n",
    "    model_res[\"mlp\"][state] = models.model_multiLayerPerceptron(*data_splits[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic view for now to see some metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from metrics import getMetrics\n",
    "model_metrics = {}\n",
    "for modelName in model_names:\n",
    "    model_metrics[modelName] = {}\n",
    "    model_metrics[modelName]['auc'] = 0.0\n",
    "    model_metrics[modelName]['acc'] = 0.0\n",
    "    model_metrics[modelName]['precision'] = 0.0\n",
    "    model_metrics[modelName]['recall'] = 0.0\n",
    "    model_metrics[modelName]['f1'] = 0.0\n",
    "\n",
    "# calculate the metrics for all models\n",
    "for rst in data_splits.keys():\n",
    "    for model in model_names:\n",
    "        metric = getMetrics(data_splits[rst][3], data_splits[rst][1], model_res[model][rst][0], model_res[model][rst][1])\n",
    "        model_metrics[model]['auc'] += metric[0]\n",
    "        model_metrics[model]['acc'] += metric[1]\n",
    "        model_metrics[model]['precision'] += metric[2]\n",
    "        model_metrics[model]['recall'] += metric[3]\n",
    "        model_metrics[model]['f1'] += metric[4]\n",
    "\n",
    "# calculate the averages for each model\n",
    "model_averages = {}\n",
    "for model in model_names:\n",
    "    model_averages[model] = {}\n",
    "    model_averages[model]['auc'] = model_metrics[model]['auc'] / len(data_splits)\n",
    "    model_averages[model]['acc'] = model_metrics[model]['acc'] / len(data_splits)\n",
    "    model_averages[model]['precision'] = model_metrics[model]['precision'] / len(data_splits)\n",
    "    model_averages[model]['recall'] = model_metrics[model]['recall'] / len(data_splits)\n",
    "    model_averages[model]['f1'] = model_metrics[model]['f1'] / len(data_splits)\n",
    "print(model_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barchart (models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeniorProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
